---
lang: zh-tw & en-us
---

# 機器學習基石(上)
###### tags: `Machine Learning Foundation`

## 目錄
- [機器學習基石(上)]()
    - [The Learning Problem](#The-Learning-Problem)
    - [Learning to Answer Yes/No](#Learning-to-Answer-Yes/No)
    - [Types of Learning](#Types-of-Learning)
    - [Feasibility of Learning](#Feasibility-of-Learning)
    - [Training versus Testing](#Training-versus-Testing)
    - [Theory of Generalization](#Theory-of-Generalization)
    - [The VC Dimension](#The-VC-Dimension)
    - [Noise and Error](#Noise-and-Error)
- [機器學習基石(下)](/n60tb-rARAG7AENmHa4rVA)


## The Learning Problem
- 學習
    - 人類學習：透過大量觀察累積之經驗來學習技能
    - 機器學習：透過大量資料累積、計算之經驗來學習技能
    - 技能：增進某一績效指標
- 為何要使用機器學習
    - 以辨識樹為例：比起把辨識樹的規則逐一寫成程式，讓機器透過大量資料來學習如何辨識樹較為簡單
    - <font color='#f00'>機器學習是建構複雜系統的另外一種方式</font>
- 機器學習的使用場景
    - 當人類無法編寫複雜的系統：火星機器人
    - 當人類無法簡單地定義規則：語音辨識
    - 當人類無法快速的下決定：股市高頻交易
    - 當需要大規模的個人化服務：客製化市場
- 機器學習的三大精隨
    - 需要有潛藏模式可以進行學習
    - 無法清楚地定義規則
    - <font color='#f00'>需要有大量資料</font>
- 數學形式化之學習問題
    - 輸入：$x\in X$
    - 輸出：$y\in Y$
    - 想要學習的潛藏模式 $\Leftrightarrow$ 目標函數：$f:X\to Y$
    - 資料 $\Leftrightarrow$ 訓練例子：$D=\lbrace (x_1,y_1), (x_2,y_2), ...,(x_n,y_n)\rbrace$
    - 假說 $\Leftrightarrow$ 技能：$g\approx f$
- 學習模型
    - 假說集合：$H$
    - 學習之演算法：$A$
    - 假說集合 $H$ 包含各種假說，由演算法 $A$ 來挑出一個假說 $g$
    - 機器學習的實際定義：<font color="#f00">使用資料來計算出假說 $g$，而其非常接近目標函數 $f$</font>
:::success
- 銀行核發信用卡之舉例
    - 銀行需要判斷使否要發信用卡給某一個顧客，而該銀行有著過去其他顧客核發信用卡的歷史資料，這個歷史資料記錄著顧客的工作或財務狀況，例如：年收入、負債、工作年資...，以及核發信用卡這個決定是否正確
    - 假定有某個未知函數是完美的核發信用卡之標準，則我們希望能藉由過去的歷史資料和各式各樣的審核標準，並透過演算法來選擇出一個審核標準，而該選定的標準應該要跟完美的標準非常接近
    - 流程圖：<center>![pic](https://i.imgur.com/kYMV3U2.png)</center>
:::
- 機器學習與資料探勘
    - 資料探勘：使用大量的資料來找出有趣的性質
    - 當有趣的性質和最終假說相同時，機器學習無異於資料探勘；當有趣的性質和最終假說類似時，機器學習和資料探勘可以互助
- 機器學習與人工智慧
    - 人工智慧：計算出有智能行為的技術
    - 最終假說非常接近目標函數時是智能行為，所以機器學習可以實現人工智慧
- 機器學習與統計學
    - 統計學：使用資料來推論未知特徵
    - 以最終假說作為推論結果、目標函數作為未知特徵，則可以透過統計方法實現機器學習

## Learning to Answer Yes/No
- 假說集合 $H$
    - 之前我們提到要從假說集合 $H$ 中挑出一個假說，但卻沒有詳細描述假說集合的數學樣貌，原因在於假說集合可能因問題的類型而有所不同，接下來要介紹一種針對二元分類的假說集合：感知器 (Perceptron)
- 感知器 (Perceptron)
    - 一筆資料若有 $d$ 個特徵，則可以把它轉換成 $d$ 維的特徵向量 $x=(x_1, x_2, ..., x_d)$，每個特徵都可以賦予不同的權重代表其對輸出的影響，形成 $d$ 維的權重向量 $w=(w_1, w_2, ..., w_d)$
    - 當特徵向量與權重向量的內積大於某閥值，輸出為 $+1$ ，否則為 $-1$ ，數學公式如下：$$h(x)=sign((\sum^d_{i=1}w_ix_i)-threshold)$$
    - 可以透過數學推導把閥值導入權重向量作為 $w_0$ ，並在特徵向量加入 $x_0=1$ ，以方便計算，如下圖：<center>![pic](https://i.imgur.com/KEqPrpK.png =500x280)</center>
    - <font color="#f00">特徵向量與權重向量的內積為一線性組合，所以特徵向量在二維會是一條直線，在多維則為超平面，可以將整個空間一分為二 $\Leftrightarrow$ 二元分類器</font>
- Perceptron Learning Algorithm (PLA)
    - 我們需要從假說集合 $H$ 中，找出一個最好的假說，PLA提出先從任一假說出發，在不斷地從錯誤中修正假說，直到沒有錯誤為止
    - 修正的方法為：$w_{t+1}\gets w_t+y_{n(t)}x_{n(t)}$
    - ==to be continued==
- PLA的相關議題
    - 不能停止的話該怎麼辦
    - 既使停下來了，其對訓練資料 $D$ 以外的樣本是否有效
- 線性可分割
    - 當PLA能夠停止時，則代表有某個權重向量 $w$ 在訓練資料 $D$ 中不會犯錯，此時 $D$ 是線性可分割的
- ==to be continued==
- Pocket Algorithm
    - 當訓練資料 $D$ 不是線性可分割的時候，我們便假定訓練樣本中是有雜訊(noise)，並不再要求所有的訓練資料都分類正確，只希望能求得一權重向量能做出最少個錯誤的分類，但這是一個NP-hard的問題
    - 在基於PLA的架構下，Pocket Algorithm會記住一個權重向量 $\hat{w}$，在不斷地從錯誤中修正權重向量的同時，$w_{t+1}$ 需要跟 $\hat{w}$ 比較分類的正確率，若 $w_{t+1}$ 有更高的正確率，則更新 $\hat{w}$ 為 $w_{t+1}$ ，直到足夠的迭代次數

## Types of Learning
- 不同輸出類型的學習問題
    - 二元分類問題：輸出的空間是二元的，而許多理論的推導、演算法都是從二元分類問題出發
    - 多元分類問題：輸出不再只有兩種可能，可以有 $k$ 種：$y=\lbrace 1, 2, ..., K\rbrace$ ，而多元分類問題有許多實際的應用，尤其是在辨識方面的領域
    - 迴歸分析：輸出的範圍是整個實數：$y=\mathbb{R}$ ，而迴歸分析在統計學的領域中被已有相當多的，所以其在統計中的工具也經常使用於機器學習
    - 結構學習：結構學習的輸入和輸出都是帶有結構的，例如：資料輸入一段中文句子，機器輸出這段句子的英文翻譯
- 不同標記類型的學習問題
    - 監督式學習：每一筆資料 $x_n$ 都有對應的標記 $y_n$
    - 非監督式學習：每一筆資料 $x_n$ 不再有對應的標記，機器必須對輸入的資料進行分類或分群
    - 半監督式學習：資料空間中的部分資料有標記，另一部份是沒有標記的，這樣的情況通常來自於資料標記的成本是昂貴的，例如：藥物測試
    - 強化學習：根據機器在環境中的行為給予獎勵或懲罰，機器在根據這些回饋修正自身行為，使機器能夠有最好的表現
- 不同溝通方式的學習問題
    - 批量學習：將資料一整批的送給機器去學習，希望機器的表現能夠很好，此為最常見的方式
    - 線上學習：將資料一個一個有序列地送給機器去學習，希望機器的表現能夠越變越好
    - 主動學習：機器主動地對不清楚的資料提出問題，這種學習方法通常應用在資料標記成本相當昂貴的情況下
- 不同輸入類型的學習問題
    - 具體特徵：特徵的表現經過人類知識的預先處理，而具體特徵對機器來說是最容易理解和使用的，例如：分類硬幣時，硬幣的尺寸和重量
    - 原始特徵：特徵的表現較為原始，通常需要人類或機器把特徵具體化，例如：數字辨識時，人類的手稿
    - 抽象特徵：特徵的表現是抽象且沒有實際物理意義的，通常需要人類或機器把特徵轉換、提取、組織，例如：預測顧客的願望清單時，該顧客的ID和曾經購買過的商品ID

## Feasibility of Learning
## Training versus Testing
## Theory of Generalization
## The VC Dimension
## Noise and Error