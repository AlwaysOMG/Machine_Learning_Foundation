---
lang: zh-tw & en-us
---

# 機器學習基石(上)
###### tags: `Machine Learning Foundation`

## 目錄
- [機器學習基石(上)]()
    - [The Learning Problem](#The-Learning-Problem)
    - [Learning to Answer Yes/No](#Learning-to-Answer-Yes/No)
    - [Types of Learning](#Types-of-Learning)
    - [Feasibility of Learning](#Feasibility-of-Learning)
    - [Training versus Testing](#Training-versus-Testing)
    - [Theory of Generalization](#Theory-of-Generalization)
    - [The VC Dimension](#The-VC-Dimension)
    - [Noise and Error](#Noise-and-Error)
- [機器學習基石(下)](/n60tb-rARAG7AENmHa4rVA)


## The Learning Problem
- 學習
    - 人類學習：透過大量觀察累積之經驗來學習技能
    - 機器學習：透過大量資料累積、計算之經驗來學習技能
    - 技能：增進某一績效指標
- 為何要使用機器學習
    - 以辨識樹為例：比起把辨識樹的規則逐一寫成程式，讓機器透過大量資料來學習如何辨識樹較為簡單
    - <font color='#f00'>機器學習是建構複雜系統的另外一種方式</font>
- 機器學習的使用局面
    - 當人類無法編寫複雜的系統：火星機器人
    - 當人類無法簡單地定義規則：語音辨識
    - 當人類無法快速的下決定：股市高頻交易
    - 當需要大規模的個人化服務：客製化市場
- 機器學習的三大精隨
    - 需要有潛藏模式可以進行學習
    - 無法清楚地定義規則
    - <font color='#f00'>需要有大量資料</font>
- 數學形式化之學習問題
    - 輸入：$x\in X$
    - 輸出：$y\in Y$
    - 想要學習的潛藏模式 $\Leftrightarrow$ 目標函數：$f:X\to Y$
    - 資料 $\Leftrightarrow$ 訓練例子：$D=\lbrace (x_1,y_1), (x_2,y_2), ...,(x_n,y_n)\rbrace$
    - 假說 $\Leftrightarrow$ 技能：$g\approx f$
- 學習模型
    - 假說集合：$H$
    - 學習之演算法：$A$
    - 假說集合 $H$ 包含各種假說，由演算法 $A$ 來挑出一個假說 $g$
    - 機器學習的實際定義：<font color="#f00">使用資料來計算出假說 $g$，而其非常接近目標函數 $f$</font>
:::success
- 銀行核發信用卡之舉例
    - 銀行需要判斷使否要發信用卡給某一個顧客，而該銀行有著過去其他顧客核發信用卡的歷史資料，這個歷史資料記錄著顧客的工作或財務狀況，例如：年收入、負債、工作年資...，以及核發信用卡這個決定是否正確
    - 假定有某個未知函數是完美的核發信用卡之標準，則我們希望能藉由過去的歷史資料和各式各樣的審核標準，並透過演算法來選擇出一個審核標準，而該選定的標準應該要跟完美的標準非常接近
    - 流程圖：<center>![pic](https://i.imgur.com/kYMV3U2.png)</center>
:::
- 機器學習與資料探勘
    - 資料探勘：使用大量的資料來找出有趣的性質
    - 當有趣的性質和最終假說相同時，機器學習無異於資料探勘；當有趣的性質和最終假說類似時，機器學習和資料探勘可以互助
- 機器學習與人工智慧
    - 人工智慧：計算出有智能行為的技術
    - 最終假說非常接近目標函數時是智能行為，所以機器學習可以實現人工智慧
- 機器學習與統計學
    - 統計學：使用資料來推論未知特徵
    - 以最終假說作為推論結果、目標函數作為未知特徵，則可以透過統計方法實現機器學習

## Learning to Answer Yes/No
- 假說集合 $H$
    - 之前提到我們要從假說集合 $H$
- 感知器 (Perceptron)
    - 一筆資料若有 $d$ 個屬性，則可以把它轉換成 $d$ 維的向量 $x=(x_1, x_2, ..., x_d)$
    - 感知器為 $d$ 維向量的權重和一閥值之組合，資料與權重兩向量的內積若大於閥值，則觸發(+)，否則不觸發(-)
    - <font color="#f00">其實感知器就是假說集合 $H$ 中的任一假說 $h$ </font>
    - 可以透過數學推導把閥值導入權重，如下圖：<center>![pic](https://i.imgur.com/KEqPrpK.png =500x280)</center>
    - 感知器與資料的內積為一線性組合，所以其在二維是一條直線(在多維則為超平面)，可以將整個空間一分為二 $\Leftrightarrow$ 二元分類器
- Perceptron Learning Algorithm (PLA)
    - 我們需要從假說集合 $H$ 中，找出一個最好的假說，PLA提出先從任一假說出發，再不斷地從錯誤中修正假說
    - 修正的方法為：$w_{t+1}\gets w_t+y_{n(t)}x_{n(t)}$
    - PLA的相關議題
        - 不能停止的話該怎麼辦呢
        - 既使停下來了，其對訓練資料 $D$ 以外的樣本是否有效呢
- 線性可分割
    - 當PLA能夠停止時，則代表有某個感知器在資料空間 $D$ 中不會犯錯，此時 $D$ 是線性可分割的
- 


## Types of Learning
- 二元分類問題
    - 許多理論的推導、演算法都是從二元分類問題出發
- 多元分類問題
    - 輸出不再只有兩種可能，可以有 $k$ 種：$y=\lbrace 1, 2, ..., K\rbrace$
    - 多元分類問題有許多實際的應用，尤其是在視覺辨識、影像辨識
- 迴歸分析
    - 迴歸分析中，輸出的範圍是整個實數：$y=\mathbb{R}$
    - 迴歸分析在統計學的領域中被已有相當多的，所以其在統計中的工具也經常使用於機器學習
- 結構學習
    - 結構學習的輸入和輸出都是帶有結構的
    - 相關應用：蛋白質折疊、演講解析樹
-

## Feasibility of Learning
## Training versus Testing
## Theory of Generalization
## The VC Dimension
## Noise and Error